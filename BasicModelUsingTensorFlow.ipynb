{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOottJYItOrpzSc402sRapu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raja978-spec/Machine-Learning/blob/main/BasicModelUsingTensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRjd9YGbA6x0",
        "outputId": "3f6c2b60-8d6c-440c-9e5a-1489d54b2315"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.17.1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load sample dataset\n",
        "# Mnist is the collection of handwritten digits\n",
        "# All the digits are represents images\n",
        "mins = tf.keras.datasets.mnist\n",
        "\n",
        "# x and y train will have the handwritten digits(images) that are\n",
        "# with their respective lables, x and y test have the same\n",
        "# values which used to evaluate model's prediction\n",
        "(x_train,y_train), (x_test,y_test) = mins.load_data()\n",
        "\n",
        "# Normalizes the pixels values in train and test\n",
        "# By divding the each pixel values in train and test\n",
        "# the value will be in between 0 and 1, 0 represents black\n",
        "# and 1 represents white.\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DbeM6P2IzJB",
        "outputId": "310af0b0-101f-4a08-b6c6-70db3cd3dbad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequential model is a linear stack of layers where you can simply\n",
        "# add one layer at a time.\n",
        "\n",
        "# The firt layer of the model is flatten which helps to change\n",
        "# the 2D array (which has the shape of 28*28) to 1D array\n",
        "# like 28 * 28 = 784, because Dense layer performs operations on\n",
        "# a flat vector of input values.\n",
        "# It requires a 1D array for each input instance.\n",
        "# This is because each neuron in the Dense layer is connected\n",
        "# to every input value, and these connections are represented\n",
        "# as a vector of weights.\n",
        "\n",
        "# Second layer is the dense layer where each neuron is connected\n",
        "# other layers, so it is called fully connected layer\n",
        "\n",
        "# Thirs is dropout used to avoid overfitting\n",
        "# 20% of the neurons will be randomly \"dropped\"\n",
        "# The dropout rate 0.2 means that 20% of the neurons\n",
        "# in the layer where the Dropout is applied will be\n",
        "# deactivated (ignored) in each forward pass during training.\n",
        "\n",
        "# The fourth layer is the output layer which has no activations\n",
        "# it will give 10 neurons as output each represents the probabilty\n",
        "# of the input belonging to a specific class\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28,28)),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])\n"
      ],
      "metadata": {
        "id": "XkiuYWcuPQBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We want output of the model should ne probability that is 0 and 1\n",
        "# for that from_logits=True is used here, it applies softmax function\n",
        "# internally to get the probabilities in 0's and 1's if it is false\n",
        "# the output will be in positive and negative integers.\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "xUxsVVmlWAsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the loss for first training example\n",
        "# Loss functions accepts two vectors predected labels,\n",
        "# actual labels, since loss function compares the\n",
        "# predicted labels with the actual labels\n",
        "loss(x_train[:1], predictions).numpy()"
      ],
      "metadata": {
        "id": "NMdpF0DoYiFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# before trainig the model we have to compile the model\n",
        "# using compile, if we are not compile it then the TensorFlow\n",
        "# won't know what loss function to\n",
        "# minimize or which optimizer to use for updating the weights.\n",
        "# hence the fit() method for training the model will raises error.\n",
        "\n",
        "\n",
        "# In this model the loss_fn will be reduced using adam optim\n",
        "# The 'accuracy' metric computes the percentage of correct predictions\n",
        "# out of all predictions.\n",
        "# During Training After each batch or epoch, TensorFlow\n",
        "# calculates the specified metric(s) based on the predictions\n",
        "# and true labels.\n",
        "# It provides feedback (e.g., accuracy) so you can monitor\n",
        "# how well the model is learning.\n",
        "# Metrics are just for monitoring and are not used to compute\n",
        "# gradients or update the model's weights. Only the loss function\n",
        "# is used for optimization.\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
        "\n",
        "# Different Types of Metrics:\n",
        "\n",
        "# The metrics you specify depend on the type of problem you're solving:\n",
        "\n",
        "# For Classification Problems:\n",
        "\n",
        "# 'accuracy': Compares predicted labels to true labels.\n",
        "# 'sparse_categorical_accuracy': Similar to 'accuracy' but used when true labels are integers (e.g., [0, 1, 2]).\n",
        "# 'categorical_accuracy': Used when true labels are one-hot encoded (e.g., [[1,0,0], [0,1,0]]).\n",
        "# For Regression Problems:\n",
        "\n",
        "# 'mean_absolute_error' (MAE): Average of the absolute differences between predictions and true values.\n",
        "# 'mean_squared_error' (MSE): Average of the squared differences between predictions and true values.\n",
        "\n",
        "# Custom Metrics:\n",
        "\n",
        "# You can define your own metrics by writing a custom function. For example:\n",
        "# def custom_metric(y_true, y_pred):\n",
        "#     return tf.reduce_mean(tf.abs(y_true - y_pred))  # Example: MAE\n",
        "\n",
        "# model.compile(optimizer='adam', loss='mse', metrics=[custom_metric])\n",
        "# Why Use Metrics?\n",
        "# Training Monitoring: Metrics give you insights into how the model is performing during training (e.g., accuracy or loss trends).\n",
        "# Validation Performance: They help you compare the performance on the training and validation datasets to detect overfitting or underfitting.\n",
        "# Evaluation: After training, metrics show how well the model performs on unseen test data.\n",
        "# Example:\n",
        "# model.compile(\n",
        "#     optimizer='adam',\n",
        "#     loss='sparse_categorical_crossentropy',\n",
        "#     metrics=['accuracy', 'sparse_categorical_accuracy']\n",
        "# )\n",
        "# accuracy: General accuracy metric.\n",
        "# sparse_categorical_accuracy: Equivalent to accuracy when using integer labels (for datasets like MNIST).\n"
      ],
      "metadata": {
        "id": "bzrpsqDEa0JW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x_train is the input data which typically consists of\n",
        "# a set of features or images.\n",
        "# y_train These are the corresponding labels for the training data.\n",
        "# The model learns to associate the input data\n",
        "# x_train with these labels (y_train) during training.\n",
        "# An epoch is one complete pass through the entire training data set.\n",
        "# The epochs parameter specifies how many times the model will\n",
        "# iterate over the entire data set during the training.\n",
        "model.fit(x_train, y_train, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZRPiTPYepNW",
        "outputId": "e142a613-c7dd-4e89-ae12-52bb2ba0d1c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.8597 - loss: 0.4802\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9533 - loss: 0.1527\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9684 - loss: 0.1061\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.9724 - loss: 0.0897\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9784 - loss: 0.0716\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f34e13c7f90>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now the model is trained, we have to evaluate the\n",
        "# model by parsing the test data set to know\n",
        "# how it predicts the data\n",
        "model.evaluate(x_test,y_test, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4krelemf7Ny",
        "outputId": "6e6a7860-6d0b-467c-d670-840aa306fd4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 1s - 2ms/step - accuracy: 0.9785 - loss: 0.0710\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.07096266746520996, 0.9785000085830688]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}